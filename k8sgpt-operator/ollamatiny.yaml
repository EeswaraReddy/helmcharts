apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          command: ["/bin/sh", "-c"]
          args:
            - |
              ollama serve &
              sleep 5
              ollama pull tinyllama
              sleep 5
              curl -X POST http://localhost:11434/api/generate \
                -H "Content-Type: application/json" \
                -d '{"model": "tinyllama", "prompt": "Hello", "stream": false}'
              tail -f /dev/null
          ports:
            - containerPort: 11434
          resources:
            limits:
              cpu: "4"
              memory: "6Gi"
            requests:
              cpu: "2"
              memory: "4Gi"
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: ollama
spec:
  selector:
    app: ollama
  ports:
    - port: 11434
      targetPort: 11434
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-egress-to-ollama
  namespace: dev-raycluster
spec:
  podSelector:
    matchLabels:
      app: jupyterhub
      component: singleuser-server
      release: jupyter
  policyTypes:
    - Egress
  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              name: ollama
        - podSelector:
            matchLabels:
              app: ollama
      ports:
        - protocol: TCP
          port: 11434
